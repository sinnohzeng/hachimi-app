// ---
// ğŸ“˜ æ–‡ä»¶è¯´æ˜ï¼š
// LLM æ¨ç†æœåŠ¡ â€” å°è£… flutter_llama çš„æ¨ç†å¼•æ“ã€‚
// æ¨ç†åœ¨ native å•çº¿ç¨‹ executor è¿è¡Œï¼Œä¸é˜»å¡ Dart UI çº¿ç¨‹ã€‚
//
// ğŸ“‹ ç¨‹åºæ•´ä½“ä¼ªä»£ç ï¼ˆä¸­æ–‡ï¼‰ï¼š
// 1. ä½¿ç”¨ FlutterLlama.instance å•ä¾‹åŠ è½½ GGUF æ¨¡å‹ï¼›
// 2. æš´éœ² generate() ç”¨äºä¸€æ¬¡æ€§æ–‡æœ¬ç”Ÿæˆï¼ˆæ—¥è®°ï¼‰ï¼›
// 3. æš´éœ² generateStream() ç”¨äºæµå¼ token è¾“å‡ºï¼ˆèŠå¤©ï¼‰ï¼›
// 4. ç®¡ç†æ¨¡å‹åŠ è½½/å¸è½½ç”Ÿå‘½å‘¨æœŸï¼›
//
// ğŸ§© æ–‡ä»¶ç»“æ„ï¼š
// - LlmServiceï¼šæ¨ç†å¼•æ“å°è£…ï¼›
// - LlmEngineStatusï¼šå¼•æ“çŠ¶æ€æšä¸¾ï¼›
//
// ğŸ•’ åˆ›å»ºæ—¶é—´ï¼š2026-02-19
// ---

import 'dart:async';

import 'package:flutter_llama/flutter_llama.dart';
import 'package:hachimi_app/core/constants/llm_constants.dart';

/// LLM å¼•æ“çŠ¶æ€ã€‚
enum LlmEngineStatus {
  unloaded,
  loading,
  ready,
  generating,
  error,
}

/// LLM æ¨ç†æœåŠ¡ â€” native å±‚å•çº¿ç¨‹æ¨ç†ï¼Œä¸é˜»å¡ UIã€‚
class LlmService {
  final FlutterLlama _llama = FlutterLlama.instance;
  LlmEngineStatus _status = LlmEngineStatus.unloaded;
  String? _lastError;

  /// å½“å‰å¼•æ“çŠ¶æ€ã€‚
  LlmEngineStatus get status => _status;

  /// æœ€è¿‘çš„é”™è¯¯ä¿¡æ¯ã€‚
  String? get lastError => _lastError;

  /// æ¨¡å‹æ˜¯å¦å·²åŠ è½½ä¸”å°±ç»ªã€‚
  bool get isReady => _status == LlmEngineStatus.ready;

  /// æ˜¯å¦æ­£åœ¨ç”Ÿæˆä¸­ã€‚
  bool get isGenerating => _status == LlmEngineStatus.generating;

  /// åŠ è½½æ¨¡å‹åˆ°å†…å­˜ã€‚
  Future<void> loadModel(String modelPath) async {
    if (_status == LlmEngineStatus.loading) return;
    if (_status == LlmEngineStatus.ready) return;

    _status = LlmEngineStatus.loading;
    _lastError = null;

    try {
      final config = LlamaConfig(
        modelPath: modelPath,
        nThreads: 4,
        nGpuLayers: 0, // CPU only â€” å®‰å…¨å…¼å®¹æ‰€æœ‰è®¾å¤‡
        contextSize: LlmConstants.contextSize,
        batchSize: 512,
        useGpu: false,
        verbose: false,
      );

      final success = await _llama.loadModel(config);
      if (success) {
        _status = LlmEngineStatus.ready;
      } else {
        _status = LlmEngineStatus.error;
        _lastError = 'Failed to load model';
      }
    } catch (e) {
      _status = LlmEngineStatus.error;
      _lastError = e.toString();
      rethrow;
    }
  }

  /// ä¸€æ¬¡æ€§æ–‡æœ¬ç”Ÿæˆï¼ˆç”¨äºæ—¥è®°ï¼‰ã€‚
  /// è¿”å›å®Œæ•´ç”Ÿæˆæ–‡æœ¬ã€‚
  Future<String> generate(String prompt) async {
    if (!_llama.isModelLoaded || _status != LlmEngineStatus.ready) {
      throw StateError('LLM engine not ready. Current status: $_status');
    }

    _status = LlmEngineStatus.generating;
    try {
      final params = GenerationParams(
        prompt: prompt,
        temperature: LlmConstants.temperature,
        topP: LlmConstants.topP,
        maxTokens: LlmConstants.diaryMaxTokens,
        repeatPenalty: LlmConstants.repeatPenalty,
        stopSequences: const ['<|im_end|>', '<|endoftext|>'],
      );

      final response = await _llama.generate(params);
      _status = LlmEngineStatus.ready;
      return _cleanResponse(response.text);
    } catch (e) {
      _status = LlmEngineStatus.ready;
      _lastError = e.toString();
      rethrow;
    }
  }

  /// æµå¼æ–‡æœ¬ç”Ÿæˆï¼ˆç”¨äºèŠå¤©ï¼‰ã€‚
  /// è¿”å› token streamï¼Œæ¯ä¸ªäº‹ä»¶æ˜¯ä¸€ä¸ª tokenã€‚
  /// è°ƒç”¨æ–¹éœ€è‡ªè¡Œæ”¶é›† token æ‹¼æ¥å®Œæ•´å›å¤ã€‚
  Stream<String> generateStream(String prompt) {
    if (!_llama.isModelLoaded || _status != LlmEngineStatus.ready) {
      return Stream.error(
        StateError('LLM engine not ready. Current status: $_status'),
      );
    }

    _status = LlmEngineStatus.generating;

    final params = GenerationParams(
      prompt: prompt,
      temperature: LlmConstants.temperature,
      topP: LlmConstants.topP,
      maxTokens: LlmConstants.chatMaxTokens,
      repeatPenalty: LlmConstants.repeatPenalty,
      stopSequences: const ['<|im_end|>', '<|endoftext|>'],
    );

    // å°†åŸå§‹ stream åŒ…è£…ä»¥ç®¡ç†çŠ¶æ€
    final controller = StreamController<String>();
    _llama.generateStream(params).listen(
      (token) {
        controller.add(token);
      },
      onError: (e) {
        _status = LlmEngineStatus.ready;
        _lastError = e.toString();
        controller.addError(e);
      },
      onDone: () {
        _status = LlmEngineStatus.ready;
        controller.close();
      },
    );

    return controller.stream;
  }

  /// åœæ­¢å½“å‰æ¨ç†ã€‚
  Future<void> stopGeneration() async {
    try {
      await _llama.stopGeneration();
    } catch (_) {
      // ignore stop errors
    }
    if (_status == LlmEngineStatus.generating) {
      _status = LlmEngineStatus.ready;
    }
  }

  /// å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜ã€‚
  Future<void> unloadModel() async {
    try {
      await _llama.unloadModel();
    } catch (_) {
      // ignore unload errors
    }
    _status = LlmEngineStatus.unloaded;
  }

  /// æ¸…ç†ç”Ÿæˆæ–‡æœ¬ä¸­çš„ç‰¹æ®Š token æ ‡è®°ã€‚
  String _cleanResponse(String text) {
    return text
        .replaceAll('<|im_end|>', '')
        .replaceAll('<|im_start|>', '')
        .replaceAll('<|endoftext|>', '')
        .trim();
  }
}
