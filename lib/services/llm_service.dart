// ---
// ğŸ“˜ æ–‡ä»¶è¯´æ˜ï¼š
// LLM æ¨ç†æœåŠ¡ â€” å°è£… llama_cpp_dart çš„ Isolate æ¨ç†å¼•æ“ã€‚
// æ¨ç†åœ¨ç‹¬ç«‹ Isolate è¿è¡Œï¼ˆdart:ffiï¼‰ï¼Œä¸é˜»å¡ Dart UI çº¿ç¨‹ã€‚
//
// ğŸ“‹ ç¨‹åºæ•´ä½“ä¼ªä»£ç ï¼ˆä¸­æ–‡ï¼‰ï¼š
// 1. ä½¿ç”¨ LlamaParent åˆ›å»ºç‹¬ç«‹ Isolate åŠ è½½ GGUF æ¨¡å‹ï¼›
// 2. æš´éœ² generate() ç”¨äºä¸€æ¬¡æ€§æ–‡æœ¬ç”Ÿæˆï¼ˆæ—¥è®°ï¼‰ï¼›
// 3. æš´éœ² generateStream() ç”¨äºæµå¼ token è¾“å‡ºï¼ˆèŠå¤©ï¼‰ï¼›
// 4. ç®¡ç†æ¨¡å‹åŠ è½½/å¸è½½ç”Ÿå‘½å‘¨æœŸï¼›
// 5. Isolate å´©æºƒæ—¶è‡ªåŠ¨é‡ç½®çŠ¶æ€ï¼Œå…è®¸é‡æ–°åŠ è½½ï¼›
//
// ğŸ§© æ–‡ä»¶ç»“æ„ï¼š
// - LlmServiceï¼šæ¨ç†å¼•æ“å°è£…ï¼›
// - LlmEngineStatusï¼šå¼•æ“çŠ¶æ€æšä¸¾ï¼›
//
// ğŸ•’ åˆ›å»ºæ—¶é—´ï¼š2026-02-19
// ---

import 'dart:async';
import 'dart:io' show Platform;

import 'package:llama_cpp_dart/llama_cpp_dart.dart';
import 'package:hachimi_app/core/constants/llm_constants.dart';

/// LLM å¼•æ“çŠ¶æ€ã€‚
enum LlmEngineStatus {
  unloaded,
  loading,
  ready,
  generating,
  error,
}

/// LLM æ¨ç†æœåŠ¡ â€” Isolate éš”ç¦»æ¨ç†ï¼Œä¸é˜»å¡ UIã€‚
class LlmService {
  LlamaParent? _parent;
  LlmEngineStatus _status = LlmEngineStatus.unloaded;
  String? _lastError;

  /// å½“å‰å¼•æ“çŠ¶æ€ã€‚
  LlmEngineStatus get status => _status;

  /// æœ€è¿‘çš„é”™è¯¯ä¿¡æ¯ã€‚
  String? get lastError => _lastError;

  /// æ¨¡å‹æ˜¯å¦å·²åŠ è½½ä¸”å°±ç»ªã€‚
  bool get isReady => _status == LlmEngineStatus.ready;

  /// æ˜¯å¦æ­£åœ¨ç”Ÿæˆä¸­ã€‚
  bool get isGenerating => _status == LlmEngineStatus.generating;

  /// åŠ è½½æ¨¡å‹åˆ°å†…å­˜ã€‚
  Future<void> loadModel(String modelPath) async {
    if (_status == LlmEngineStatus.loading) return;
    if (_status == LlmEngineStatus.ready) return;

    // iOS æš‚ä¸æ”¯æŒï¼ˆllama_cpp_dart æ— é¢„ç¼–è¯‘ xcframeworkï¼‰
    if (Platform.isIOS) {
      _status = LlmEngineStatus.error;
      _lastError = 'AI features coming soon on iOS';
      return;
    }

    _status = LlmEngineStatus.loading;
    _lastError = null;

    try {
      final loadCommand = LlamaLoad(
        path: modelPath,
        modelParams: ModelParams()..nGpuLayers = 0, // CPU only
        contextParams: ContextParams()
          ..nCtx = LlmConstants.contextSize
          ..nBatch = 512
          ..nThreads = 4
          ..nPredict = LlmConstants.diaryMaxTokens,
        samplingParams: SamplerParams()
          ..temp = LlmConstants.temperature
          ..topP = LlmConstants.topP
          ..penaltyRepeat = LlmConstants.repeatPenalty,
        verbose: false,
      );

      _parent = LlamaParent(loadCommand);
      await _parent!.init();
      _status = LlmEngineStatus.ready;
    } catch (e) {
      _status = LlmEngineStatus.error;
      _lastError = e.toString();
      _parent = null;
      rethrow;
    }
  }

  /// ä¸€æ¬¡æ€§æ–‡æœ¬ç”Ÿæˆï¼ˆç”¨äºæ—¥è®°ï¼‰ã€‚
  /// è¿”å›å®Œæ•´ç”Ÿæˆæ–‡æœ¬ã€‚
  Future<String> generate(String prompt) async {
    final parent = _parent;
    if (parent == null || _status != LlmEngineStatus.ready) {
      throw StateError('LLM engine not ready. Current status: $_status');
    }

    _status = LlmEngineStatus.generating;
    try {
      final buffer = StringBuffer();
      StreamSubscription<String>? streamSub;

      final completer = Completer<String>();

      streamSub = parent.stream.listen(
        (token) {
          buffer.write(token);
        },
        onError: (e) {
          if (!completer.isCompleted) {
            completer.completeError(e);
          }
        },
      );

      // ç›‘å¬å®Œæˆäº‹ä»¶
      StreamSubscription<CompletionEvent>? completionSub;
      completionSub = parent.completions.listen((event) {
        if (!completer.isCompleted) {
          completer.complete(buffer.toString());
        }
        completionSub?.cancel();
      });

      await parent.sendPrompt(prompt);

      final result = await completer.future;
      await streamSub.cancel();
      await completionSub.cancel();

      _status = LlmEngineStatus.ready;
      return _cleanResponse(result);
    } catch (e) {
      _resetOnError(e);
      rethrow;
    }
  }

  /// æµå¼æ–‡æœ¬ç”Ÿæˆï¼ˆç”¨äºèŠå¤©ï¼‰ã€‚
  /// è¿”å› token streamï¼Œæ¯ä¸ªäº‹ä»¶æ˜¯ä¸€ä¸ª tokenã€‚
  /// è°ƒç”¨æ–¹éœ€è‡ªè¡Œæ”¶é›† token æ‹¼æ¥å®Œæ•´å›å¤ã€‚
  Stream<String> generateStream(String prompt) {
    final parent = _parent;
    if (parent == null || _status != LlmEngineStatus.ready) {
      return Stream.error(
        StateError('LLM engine not ready. Current status: $_status'),
      );
    }

    _status = LlmEngineStatus.generating;

    final controller = StreamController<String>();

    // ç›‘å¬ token stream
    StreamSubscription<String>? streamSub;
    StreamSubscription<CompletionEvent>? completionSub;

    streamSub = parent.stream.listen(
      (token) {
        controller.add(token);
      },
      onError: (e) {
        _resetOnError(e);
        controller.addError(e);
      },
    );

    completionSub = parent.completions.listen((event) {
      _status = LlmEngineStatus.ready;
      streamSub?.cancel();
      completionSub?.cancel();
      controller.close();
    });

    // å‘é€ prompt
    parent.sendPrompt(prompt).catchError((Object e) {
      _resetOnError(e);
      controller.addError(e);
      streamSub?.cancel();
      completionSub?.cancel();
      controller.close();
      return ''; // satisfy return type
    });

    controller.onCancel = () {
      streamSub?.cancel();
      completionSub?.cancel();
    };

    return controller.stream;
  }

  /// åœæ­¢å½“å‰æ¨ç†ã€‚
  Future<void> stopGeneration() async {
    try {
      await _parent?.stop();
    } catch (_) {
      // ignore stop errors
    }
    if (_status == LlmEngineStatus.generating) {
      _status = LlmEngineStatus.ready;
    }
  }

  /// å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾å†…å­˜ã€‚
  Future<void> unloadModel() async {
    try {
      await _parent?.dispose();
    } catch (_) {
      // ignore dispose errors
    }
    _parent = null;
    _status = LlmEngineStatus.unloaded;
  }

  /// Isolate å´©æºƒæˆ–å¼‚å¸¸æ—¶é‡ç½®çŠ¶æ€ï¼Œå…è®¸é‡æ–° loadModel()ã€‚
  void _resetOnError(Object e) {
    _status = LlmEngineStatus.error;
    _lastError = e.toString();
    // Isolate å¯èƒ½å·²æ­»ï¼Œç½®ç©ºå…è®¸é‡æ–°åŠ è½½
    _parent = null;
  }

  /// æ¸…ç†ç”Ÿæˆæ–‡æœ¬ä¸­çš„ç‰¹æ®Š token æ ‡è®°ã€‚
  String _cleanResponse(String text) {
    return text
        .replaceAll('<|im_end|>', '')
        .replaceAll('<|im_start|>', '')
        .replaceAll('<|endoftext|>', '')
        .trim();
  }
}
